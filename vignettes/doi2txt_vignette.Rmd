---
title: "Using doi2txt to extract sections of full text articles"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using doi2txt to extract sections of full text articles}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Ideas and notes

Is this the proper use of a vignette? No, but it seems like the easiest way to work together on getting the package built and tested since it is easy to leave notes and ideas here with examples.

## lapply and using lists

One idea I (Eliza) had is to make all the functions default to using lapply since we don't know how many items will be passed to any given function. This would put us solidly in the realm of lists as our primary way of working with data, which I think is technically not following tidy principles. It may be unavoidable though, since all of the text sections will be of variable length. We could collapse them into one large character block with manual line breaks though, if people think it would be best to return data.frames from functions. Thoughts?

## tidy?

Relatedly, how tidy do we want to be? Assuming this gets wrapped up in the metaverse, then I think we are supposed to aim for following tidy principles because of funding for that project. Martin, does it matter on your end?

## Dealing with odd errors

When working with scraped data, we will inevitably encounter lots of errors that we did not plan for and are not built into the functions to cope with. My suggestion for this, which is currently implemented in a couple functions, is to use try() to catch error messages, and then replace any resulting error messages with NA. Does anyone know of better approaches for this problem?

Gabby:: we can use tryCatch() which allows us to set different actions for warnings, msgs, errors, and interrupts. And it also lets us make more informative error messages. 



# "Clean" example that makes use of wrapper functions

This first section is the "clean" version of the first few steps in the package, meaning it uses the wrapper functions which is presumably what most users will want to do. The idea for this section is just to have some example DOIs, download the associated HTML journal articles, and extract the methods and references.

```{r}
library(doi2txt)

# Example DOIs used for function testing
# Should all be open access, if I recall correctly

#dois <- get_dastardly_dois(limit=100)
#dois <- dois[!is.na(dois)]

# results of running the above code; the function is random so now we have persistent dois
dois <- c("10.5281/zenodo.1418716", "10.22004/ag.econ.22278", "10.5281/zenodo.2575479", 
"10.23670/irj.2018.78.12.035", "10.22004/ag.econ.22830", "10.22004/ag.econ.131625", 
"10.5281/zenodo.1285283", "10.24425/pjvs.2019.129980", "10.22004/ag.econ.263729", 
"10.13140/rg.2.2.30224.51207", "10.15463/rec.1189734208", "10.22004/ag.econ.159552", 
"10.1364/oe.26.031454", "10.1080/17454832.2017.1317004", "10.5281/zenodo.3548042", 
"10.1177/104438942800800901", "10.1016/j.shpsa.2010.10.009", 
"10.4225/03/58b7790604715")

# doi 11 is philosophy or something, fine that functions act weird
# doi 12 needs the redirect fix to full text
article <- abstract <- methods <- list()

for(i in 1:2){
  
article[[i]] <- doi2html(dois[i])
head(article[[i]], 10)

abstract[[i]] <- try(extract_section(article[[i]], section = "abstract", max_lines = 1, clean=TRUE, min_words = 50, forcestart = TRUE))

methods[[i]] <- try(extract_section(article[[i]], section = "methods", max_lines = 50, clean=TRUE, min_words=1, forcestart = FALSE))
}

i <- i +1

article <- doi2html(dois[i])
head(article, 10)


abstract <- extract_section(article, section = "abstract", max_lines = 1, clean=TRUE, min_words = 50, forcestart = TRUE)
abstract

methods <- extract_section(article, section = "methods", max_lines = 50, clean=TRUE, min_words=1, forcestart = FALSE)
methods

rm(abstract, methods, article)

```

# Problems

This is a graveyard for problems that need to be solved or complexities of downloading some articles. For example, html pages that cannot be rendered properly, hitting a sign-in screen, the actual article being an embedded pdf on an html site, needing to click through a link to get to the actual article, etc. Please be as specific as possible with problems, and preferably include an example of where doi2txt fails in regards to a particular problem with code to reproduce it. 

## Subscription issue

The third doi (10.1016/j.cja.2014.04.019) leads to a paywalled HTML. The full text HTML is there, but I'm accessing it through a proxy. Can someone with a VPN or onsite license try?

```{r}

#placeholder for problem 1 code example

```

## No HTML available

The fourth doi (10.24843/EJA.2019.v26.i01.p07) leads to an abstract but no HTML full text, just a PDF. It's for a rather obscure Indonesian language journal, so low priority.

A related issue: some DOIs (e.g. 10.23670/irj.2018.78.12.035) lead directly to a PDF and the download function fails because it expects html. We may be able to fix this by checking the redirect from doi.org and throwing in an error if the extension is .pdf. Or, they are html with an embedded PDF (e.g. 10.1177/104438942800800901).

## Mini-sections in the abstract

Some articles use section headers in the abstract, so there are two section headers detected for methods and it retrieves the first one instead of the actual methods section. May need to change how we process length and/or check the nchar of the line immediately following each detected header.

This is now fixed with commit 1ff4cba.

```{r}
doi <- "10.1002/hpm.2581"

article <- doi2html(doi)

methods <- extract_section(article, "methods")

```

